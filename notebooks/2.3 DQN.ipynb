{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "511c6d6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7f703becb940>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%capture\n",
    "# !apt-get update\n",
    "# !apt-get install -y xvfb python-opengl ffmpeg\n",
    "# !pip install pyglet==1.3.2\n",
    "# !pip install gym pyvirtualdisplay\n",
    "# !pip install torch\n",
    "# !pip install xvfbwrapper\n",
    "# pip install moviepy\n",
    "import gym\n",
    "from gym.wrappers.record_video import RecordVideo\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad32409",
   "metadata": {},
   "source": [
    "Construimos el agente DQN y algunas funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c49d172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of state features: 4\n",
      "Number of possible actions: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vania/jupyter/jupyterenv/lib/python3.8/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "num_features = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "print('Number of state features: {}'.format(num_features))\n",
    "print('Number of possible actions: {}'.format(num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36890591",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e1c8ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"Dense neural network class.\"\"\"\n",
    "    def __init__(self, num_inputs, num_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.out = nn.Linear(32, num_actions)\n",
    "\n",
    "    def forward(self, states):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        x = F.relu(self.fc1(states))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "main_nn = DQN(num_features, num_actions).to(device)\n",
    "target_nn = DQN(num_features, num_actions).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(main_nn.parameters(), lr=1e-4)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "682b3e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\"Experience replay buffer that samples uniformly.\"\"\"\n",
    "    def __init__(self, size, device=\"cpu\"):\n",
    "        \"\"\"Initializes the buffer.\"\"\"\n",
    "        self.buffer = deque(maxlen=size)\n",
    "        self.device = device\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        idx = np.random.choice(len(self.buffer), num_samples)\n",
    "        for i in idx:\n",
    "            elem = self.buffer[i]\n",
    "            state, action, reward, next_state, done = elem\n",
    "            states.append(np.array(state, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            next_states.append(np.array(next_state, copy=False))\n",
    "            dones.append(done)\n",
    "        states = torch.as_tensor(np.array(states), device=self.device)\n",
    "        actions = torch.as_tensor(np.array(actions), device=self.device)\n",
    "        rewards = torch.as_tensor(\n",
    "            np.array(rewards, dtype=np.float32), device=self.device)\n",
    "        next_states = torch.as_tensor(np.array(next_states), device=self.device)\n",
    "        dones = torch.as_tensor(np.array(dones, dtype=np.float32), device=self.device)\n",
    "        return states, actions, rewards, next_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29bb3944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_epsilon_greedy_action(state, epsilon):\n",
    "    \"\"\"Take random action with probability epsilon, else take best action.\"\"\"\n",
    "    result = np.random.uniform()\n",
    "    if result < epsilon:\n",
    "        return env.action_space.sample() \n",
    "    else:\n",
    "        qs = main_nn(state).cpu().data.numpy()\n",
    "        return np.argmax(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25b0a61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(states, actions, rewards, next_states, dones):\n",
    "    \"\"\"Perform a training iteration on a batch of data sampled from the experience\n",
    "    replay buffer.\"\"\"\n",
    "    max_next_qs = target_nn(next_states).max(-1).values\n",
    "    target = rewards + (1.0 - dones) * discount * max_next_qs\n",
    "    qs = main_nn(states)\n",
    "    action_masks = F.one_hot(actions, num_actions)\n",
    "    masked_qs = (action_masks * qs).sum(dim=-1)\n",
    "    loss = loss_fn(masked_qs, target.detach())\n",
    "    #nn.utils.clip_grad_norm_(loss, max_norm=10)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed86535c",
   "metadata": {},
   "source": [
    "Corremos el algoritmo DQN y observamos como aprende el algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b43a6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00116655, -0.04113458, -0.03134447,  0.00210042], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef02d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/1000. Epsilon: 0.999. Reward in last 100 episodes: 12.00\n",
      "Episode 50/1000. Epsilon: 0.949. Reward in last 100 episodes: 21.57\n",
      "Episode 100/1000. Epsilon: 0.899. Reward in last 100 episodes: 24.17\n",
      "Episode 150/1000. Epsilon: 0.849. Reward in last 100 episodes: 29.86\n",
      "Episode 200/1000. Epsilon: 0.799. Reward in last 100 episodes: 31.40\n",
      "Episode 250/1000. Epsilon: 0.749. Reward in last 100 episodes: 31.39\n",
      "Episode 300/1000. Epsilon: 0.699. Reward in last 100 episodes: 38.11\n",
      "Episode 350/1000. Epsilon: 0.649. Reward in last 100 episodes: 50.12\n",
      "Episode 400/1000. Epsilon: 0.599. Reward in last 100 episodes: 60.90\n",
      "Episode 450/1000. Epsilon: 0.549. Reward in last 100 episodes: 75.02\n",
      "Episode 500/1000. Epsilon: 0.499. Reward in last 100 episodes: 97.34\n",
      "Episode 550/1000. Epsilon: 0.449. Reward in last 100 episodes: 118.63\n",
      "Episode 600/1000. Epsilon: 0.399. Reward in last 100 episodes: 141.31\n",
      "Episode 650/1000. Epsilon: 0.349. Reward in last 100 episodes: 178.10\n",
      "Episode 700/1000. Epsilon: 0.299. Reward in last 100 episodes: 204.02\n",
      "Episode 750/1000. Epsilon: 0.249. Reward in last 100 episodes: 223.45\n",
      "Episode 800/1000. Epsilon: 0.199. Reward in last 100 episodes: 249.44\n",
      "Episode 850/1000. Epsilon: 0.149. Reward in last 100 episodes: 270.91\n",
      "Episode 900/1000. Epsilon: 0.099. Reward in last 100 episodes: 285.52\n",
      "Episode 950/1000. Epsilon: 0.050. Reward in last 100 episodes: 282.51\n"
     ]
    }
   ],
   "source": [
    "# Hyperparametros.\n",
    "num_episodes = 1000\n",
    "epsilon = 1.0\n",
    "batch_size = 32\n",
    "discount = 0.99\n",
    "buffer = ReplayBuffer(100000, device=device)\n",
    "cur_frame = 0\n",
    "\n",
    "# Empieza a entrenar. Juega una vez y luego entrena con un batch.\n",
    "last_100_ep_rewards = []\n",
    "for episode in range(num_episodes+1):\n",
    "    state = env.reset()[0].astype(np.float32)\n",
    "    ep_reward, done = 0, False\n",
    "    while not done:\n",
    "        state_in = torch.from_numpy(np.expand_dims(state, axis=0)).to(device)\n",
    "        action = select_epsilon_greedy_action(state_in, epsilon)\n",
    "        next_state, reward, done, info = env.step(action)[:4]\n",
    "        next_state = next_state.astype(np.float32)\n",
    "        ep_reward += reward\n",
    "        # Guardamos para reproducir la experiencia.\n",
    "        buffer.add(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        cur_frame += 1\n",
    "        # Copiamos los pesos de main_nn a target_nn.\n",
    "        if cur_frame % 2000 == 0:\n",
    "            target_nn.load_state_dict(main_nn.state_dict())\n",
    "    \n",
    "        # Entrenamos la red neuronal.\n",
    "        if len(buffer) > batch_size:\n",
    "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "            loss = train_step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "    if episode < 950:\n",
    "        epsilon -= 0.001\n",
    "\n",
    "    if len(last_100_ep_rewards) == 100:\n",
    "        last_100_ep_rewards = last_100_ep_rewards[1:]\n",
    "    last_100_ep_rewards.append(ep_reward)\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        print(f'Episode {episode}/{num_episodes}. Epsilon: {epsilon:.3f}.'\n",
    "          f' Reward in last 100 episodes: {np.mean(last_100_ep_rewards):.2f}')\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893d769b",
   "metadata": {},
   "source": [
    "Mostrar el resultado del agente DQN entrenado en el entorno Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f363aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_video():\n",
    "    \"\"\"Enables video recording of gym environment and shows it.\"\"\"\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Video not found\")\n",
    "\n",
    "def wrap_env(env):\n",
    "    env = RecordVideo(env, './video', episode_trigger = lambda episode_number: True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ac1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = wrap_env(gym.make('CartPole-v0'))\n",
    "state = env.reset()[0]\n",
    "done = False\n",
    "ep_rew = 0\n",
    "while not done:\n",
    "    env.render()\n",
    "    state = state.astype(np.float32)\n",
    "    state = torch.from_numpy(np.expand_dims(state, axis=0)).to(device)\n",
    "    action = select_epsilon_greedy_action(state, epsilon=0.01)\n",
    "    state, reward, done, info = env.step(action)[0]\n",
    "    ep_rew += reward\n",
    "print('Return on this episode: {}'.format(ep_rew))\n",
    "env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c6fa31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
